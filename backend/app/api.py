from fastapi import APIRouter, HTTPException
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
import datetime
import json
import openai
from typing import Optional, List, Dict
import asyncio

router = APIRouter()

# è¯·æ±‚æ¨¡å‹
class EchoRequest(BaseModel):
    message: str

class HelloResponse(BaseModel):
    message: str
    timestamp: str

class EchoResponse(BaseModel):
    echo: str
    original: str
    timestamp: str

@router.get("/hello", response_model=HelloResponse)
async def get_hello():
    """è·å–åç«¯ Hello æ¶ˆæ¯"""
    return HelloResponse(
        message="Hello from Python FastAPI Backend! ğŸ",
        timestamp=datetime.datetime.now().isoformat()
    )

@router.post("/echo", response_model=EchoResponse)
async def echo_message(request: EchoRequest):
    """å›æ˜¾æ¶ˆæ¯"""
    return EchoResponse(
        echo=f"æœåŠ¡å™¨æ”¶åˆ°ï¼š{request.message}",
        original=request.message,
        timestamp=datetime.datetime.now().isoformat()
    )

@router.get("/status")
async def get_status():
    """è·å–æœåŠ¡çŠ¶æ€"""
    return {
        "status": "running",
        "service": "FastAPI Backend",
        "version": "1.0.0",
        "timestamp": datetime.datetime.now().isoformat()
    }

# OpenAI ç›¸å…³æ¨¡å‹
class OpenAIConfigRequest(BaseModel):
    api_key: str
    base_url: Optional[str] = None

class ChatMessage(BaseModel):
    role: str  # "user" or "assistant"
    content: str

class ChatRequest(BaseModel):
    messages: List[ChatMessage]
    api_key: str
    base_url: Optional[str] = None
    model: str = "gpt-3.5-turbo"
    stream: bool = True

# å­˜å‚¨OpenAIé…ç½®
openai_config = {}

@router.post("/openai/config")
async def set_openai_config(config: OpenAIConfigRequest):
    """è®¾ç½®OpenAIé…ç½®"""
    global openai_config
    openai_config = {
        "api_key": config.api_key,
        "base_url": config.base_url or "https://api.openai.com/v1"
    }
    return {"status": "success", "message": "OpenAIé…ç½®å·²ä¿å­˜"}

@router.post("/openai/chat")
async def chat_with_openai(request: ChatRequest):
    """ä¸OpenAIè¿›è¡ŒèŠå¤©å¯¹è¯ï¼ˆæµå¼å“åº”ï¼‰"""
    try:
        # ä½¿ç”¨è¯·æ±‚ä¸­çš„é…ç½®æˆ–å…¨å±€é…ç½®
        api_key = request.api_key or openai_config.get("api_key")
        base_url = request.base_url or openai_config.get("base_url", "https://api.openai.com/v1")
        
        if not api_key:
            raise HTTPException(status_code=400, detail="APIå¯†é’¥æœªæä¾›")
        
        # åˆ›å»ºOpenAIå®¢æˆ·ç«¯
        client = openai.AsyncOpenAI(
            api_key=api_key,
            base_url=base_url
        )
        
        # è½¬æ¢æ¶ˆæ¯æ ¼å¼
        messages = [{"role": msg.role, "content": msg.content} for msg in request.messages]
        
        if request.stream:
            # æµå¼å“åº”
            async def generate_stream():
                try:
                    stream = await client.chat.completions.create(
                        model=request.model,
                        messages=messages,
                        stream=True,
                        temperature=0.7
                    )
                    
                    async for chunk in stream:
                        if chunk.choices and chunk.choices[0].delta and chunk.choices[0].delta.content:
                            content = chunk.choices[0].delta.content
                            # å‘é€Server-Sent Eventsæ ¼å¼çš„æ•°æ®
                            yield f"data: {json.dumps({'content': content, 'role': 'assistant'})}\n\n"
                    
                    # å‘é€ç»“æŸæ ‡è®°
                    yield "data: [DONE]\n\n"
                    
                except Exception as e:
                    error_data = {"error": str(e), "type": "api_error"}
                    yield f"data: {json.dumps(error_data)}\n\n"
            
            return StreamingResponse(
                generate_stream(),
                media_type="text/plain",
                headers={
                    "Cache-Control": "no-cache",
                    "Connection": "keep-alive",
                    "Access-Control-Allow-Origin": "*",
                    "Access-Control-Allow-Headers": "*",
                }
            )
        else:
            # éæµå¼å“åº”
            response = await client.chat.completions.create(
                model=request.model,
                messages=messages,
                temperature=0.7
            )
            
            return {
                "message": {
                    "role": "assistant",
                    "content": response.choices[0].message.content
                },
                "timestamp": datetime.datetime.now().isoformat()
            }
            
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"OpenAI APIè°ƒç”¨å¤±è´¥: {str(e)}")

@router.get("/openai/models")
async def get_available_models():
    """è·å–å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨"""
    if not openai_config.get("api_key"):
        raise HTTPException(status_code=400, detail="è¯·å…ˆé…ç½®APIå¯†é’¥")
    
    try:
        client = openai.AsyncOpenAI(
            api_key=openai_config["api_key"],
            base_url=openai_config.get("base_url", "https://api.openai.com/v1")
        )
        
        models = await client.models.list()
        return {
            "models": [model.id for model in models.data if "gpt" in model.id.lower()],
            "timestamp": datetime.datetime.now().isoformat()
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥: {str(e)}") 